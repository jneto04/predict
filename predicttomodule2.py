# -*- coding: utf-8 -*-
"""PredictToModule2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VmT3MqcpVTMvH4Vg_wt0eNoytJQhsmlq
"""

!pip install transformers==4.28.0

import pandas as pd

df_test_v3 = pd.read_csv('./to_predict_to_module_2.csv', sep='\t')

to_predict = df_test_v3['input'].values.tolist()

from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained("t5-base")
model = T5ForConditionalGeneration.from_pretrained(pretrained_model_name_or_path="/content/drive/MyDrive/UNISINOS/Journal/modulo1/").to('cuda')

count, _count = 0, 0
with open('./predicted_sentences_to_module_2.csv', mode='w+', encoding='utf-8') as f:
  f.write('input\ttarget\n')
  for txt in to_predict:
    inputs = tokenizer(txt, max_length=1024, return_tensors="pt").to('cuda')
    outputs = model.generate(max_length=1024, **inputs)
    verbalization = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    t = txt.split(' PLAIN SENTENCE: ')[0]
    x = t.split(' GraphToText: ')
    triples = ' '.join(x)
    f.write(triples+'\t'+verbalization+'\n')
    if _count == 50:
      count+=50
      print(' :: Processing: '+str(count)+'/'+str(len(to_predict)))
      _count = 0
    _count+=1
  f.close()